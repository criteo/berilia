address = jobs-user.pa4.hpc.criteo.prod
tables = [
  {
    name = log.t1
    sample.prob = 0.02
    partitions = [
      ["day='2017-05-01'","platform=2"]
    ]
  },
  {
    name = log.t2
    sample.prob = 0.1
    sample.size = 500
    skip.cleanup = true
  },
  {
    name = db.t3
    partition.count = 3
  }
]
s3.hdfs.scheme = s3a
copy {
  scheme = tunnel
  sample.threshold = 10241024
  overwriteIfExists = true
}
gateway {
  docker.ports = ["abc/9000"]
  docker.files = ["client", "zip"]
  dc1.conf = client_dc1
}
default {
  sample.prob = 0.01
  partition.count = -1
}
sample.database = sample_db
parallelism = {
  table = 10
  partition = 10
}
hadoop.conf.dir = cluster
hive.aux.jars = ["utils.jar"]
hadoop.version = cdh5.5.0
base.os = ubuntu-trusty
local {
  docker.files = ["et"]
  ports = ["ui/8080"]
  cluster.user = "root"
  docker.containerId = "d"
}
aws {
  user = ubuntu
  access.id = 123
  access.key = abc
  instance.type = m4.xlarge
  region = eu-west-1
  subnet = subnet-123
  security.group = sg
  key.pair = p1
  key.file = k.pem
  base.image.id = ami
  auto.volumes = true
  volume.spec = {
    master = [
      {
        name = "/dev/sda1"
        size = 100
      },
      {
        name = "/dev/sdb"
        size = 500
      }
    ]
    slave = [
      {
        name = "/dev/sda1"
        size = 10
      },
      {
        name = "/dev/sdb"
        size = 500
      }
    ]
  }
}
s3.bucket.prefix = dev-cluster