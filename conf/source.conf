//URL for gateway to cluster with source data.
//Requirement is that the terminal running the dev_cluster tool has valid security credentials
//to access this via password-less SSH, and may run HDFS and Hive commands on it.
address = ""

//Algorithm to copy from source to dev cluster.  Note that one could just use distcp, but there might be
//security limitations on both clusters preventing this from happening.
//"tunnel" copies to local machine and then the dev cluster.
//"bucket" copies first to a temp bucket secured with provided AWS credentials in target-aws.xml, and then to dev cluster (in development).
//"local" copies directly local files to the dev cluster
copy {
  scheme = tunnel
  sample.threshold = 500000000
  //If set true, when a file to be copied exists, it will be overwritten
  overwriteIfExists = true
}

//Sampling properties
default {
  sample.prob = 0.01
  partition.count = -1
}
sample.database = dev_cluster_sample
s3.hdfs.scheme = s3a

//Array of "database"."table".
//For each table, partitions is optional, if it is specified, it will be passed to Hive to only fetch matching //partitions that match (up to the partition count).
//Multiple partitions may be passed, and all matching partitions will be returned (up to the partition count).
tables = [
  {
    "name" : "db.table",
    "partitions" : [
      [
        "day='2000-01-01'","hour=24"
      ]
    ]
    sample.prob = 0.01
    partition.count = 2
  }
]

//Arrayof listeners to run on copy action, ex, handle special formats
//Copies over pail.meta file.  This goes along with criteo-cluster conf,
//which sets the default Hive input format to be PailOrCombineInputFormat.
copy.listeners = []

//This alters the sample table (if a glup table) to also write data in glup format to the final destination.
sample.listeners = []

//Configure gateway to access a Hadoop cluster.
gateway {
  //Array of additional ports to be opened on gateway.
  //Port specs are of the form "$port_description/$exposed_port_num:$portNum"
  //$port_description is optional and used for display in list-gateway.
  //$exposed_port_num is optional, and will provide a hard-binding.
  docker.ports = []

  //Array of additional docker files to run to set up the gateway.
  //DockerFiles must start with line "FROM dev_cluster/gateway"
  //and be located under /docker/contrib-gateway.
  docker.files = []

  //Given a command "dev-cluster gateway $mycluster", it will search for this directory to get the Hadoop/Hive client conf. The directory would be located under ./hadoop-resources/hadoop-conf/
  gateway.mycluster.conf = mycluster
}
// How many threads are used to copy data
parallelism = 1

