<?xml version="1.0" encoding="UTF-8" standalone="yes" ?>
<configuration>
    <!--DATA SOURCE PROPERTIES-->
    <property>
        <name>source.address</name>
        <value></value>
        <description>URL for gateway to cluster with source data.
            Requirement is that the terminal running the dev_cluster tool has valid security credentials
            to access this via password-less SSH, and may run HDFS and Hive commands on it.</description>
    </property>

    <property>
        <name>source.tables</name>
        <value></value>
        <description>
            Fill this with comma separated list of "database"."table" (partspec).
            Partspec is optional, if it is specified, it will be passed to Hive to only fetch partitions that match.
        </description>
    </property>

    <property>
        <name>default.partition.count</name>
        <value>2</value>
        <description>
            How many leaf partitions to copy for each table.
            A value of -1 indicates no limit to partitions.
            This config may be overriden per table, via $db.$table.partition.count property.
        </description>
    </property>

    <property>
        <name>default.sample.prob</name>
        <value>0.2</value>
        <description>
            This controls how many rows we sample when we copy a partition/table.  Must be between 0(exc) and 1(inclusive)

            If copy-bucket jobs errors with "No Space Left on Disk" when writing to S3, tune this flag to control
            how much the job spills to disk while writing to S3.
        </description>
    </property>

    <property>
        <name>source.sample.database</name>
        <value>dev_cluster_sample</value>
        <description>
            Name of intermediate Hive database where sampled source tables will live, if sampling is enabled during copy.
            This needs to be created before the copy command is run.
        </description>
    </property>

    <!--To override default partition and sampling configs for particular "my_db"."my_table",
     provide property with key: "my_db"."my_table".partition.count, eg: -->
    <property>
        <name>my_db.my_table.partition.count</name>
        <value>1</value>
        <description>Table is a bit big, so copy only one partition.</description>
    </property>

    <property>
        <name>my_db.my_table.sample.rows</name>
        <value>200000</value>
        <description>Override sampling in that partition.</description>
    </property>

    <property>
        <name>source.files</name>
        <value></value>
        <description>Fill this with comma-separated list of source directories.  TODO: file sampling</description>
    </property>

    <!--PROFILE EXAMPLE-->
    <profile>
        <id>sample-1</id>
        <default>false</default>
        <description>You may create profiles in this file like this to specify groups of files and tables.
            This would be invoked via specifying on the command line the flag -Psample-1.</description>

        <property>
            <name>source.tables</name>
            <value>mydatabase.mytable (day='2000-01-01')</value>
        </property>

        <property>
            <name>source.files</name>
            <value>/user/myuser/directory</value>
        </property>
    </profile>



    <!-- ADVANCED PROPERTIES -->

    <!-- Comma separated list of listeners to run on copy action, ex, handle special formats -->
    <!--<property>
        <name>source.copy.listeners</name>
        <value>com.criteo.dev.cluster.copy.PailCopyListener</value>
        <description>This copies over pail.meta file.  It is only a sample and you may provide your own for your own data formats.</description>
    </property> -->

    <property>
        <name>source.copy.sample.threshold</name>
        <value>50000000000</value>
        <description>
            Threshold for aggregate size of data marked for copy in a given table above which to begin sampling.
            There is some overhead in generating the sample, but it is quicker to move it to the final destination.

            Unit is bytes.  This default is 50 Gigabytes.
        </description>
    </property>
</configuration>